{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Dense, concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Embedding</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Created_on</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>CustomName</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "      <th>UMAP_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>finally got dream lilxan account confirmed pgp...</td>\n",
       "      <td>[-0.05092696, -0.028831957, 0.03425763, -0.008...</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.0019145853509105579, 0.0034859457117316834,...</td>\n",
       "      <td>2019-10-16</td>\n",
       "      <td>2462</td>\n",
       "      <td>9_pgp_begin pgp_begin_pgp signature</td>\n",
       "      <td>pgp - pgp signature - hash signature</td>\n",
       "      <td>[pgp, begin pgp, begin, pgp signature, signatu...</td>\n",
       "      <td>[vendor links last online fri dec utc last onl...</td>\n",
       "      <td>[0.18862869, 1.9764707, 6.3155804, -0.32967466...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im issues vendor account issues withdrawing cm...</td>\n",
       "      <td>[0.0006082218, 0.061474808, 0.017827673, 0.010...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.004161525996584431, 0.09864712150395494, 0....</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>5365</td>\n",
       "      <td>1_deposit_address_ticket_btc</td>\n",
       "      <td>ticket - deposit - address</td>\n",
       "      <td>[deposit, address, ticket, btc, wallet, deposi...</td>\n",
       "      <td>[missing two deposit week ago big deposit erro...</td>\n",
       "      <td>[0.71821356, -0.18454741, 2.907359, -0.1046811...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>making switch xmr besides xmr hodler favorite ...</td>\n",
       "      <td>[-0.063973725, -0.040487997, -0.025320696, -0....</td>\n",
       "      <td>15</td>\n",
       "      <td>[0.002285542372071366, 0.007466206317565759, 0...</td>\n",
       "      <td>2019-10-16</td>\n",
       "      <td>2995</td>\n",
       "      <td>15_monero_xmr_wallet_btc</td>\n",
       "      <td>monero - btc - wallet</td>\n",
       "      <td>[monero, xmr, wallet, btc, exchange, bitcoin, ...</td>\n",
       "      <td>[mixing bitcoin old school easy get scammed fe...</td>\n",
       "      <td>[0.15870488, 0.024624836, 3.1060727, -0.176952...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>got free cooky cart order cannacreations one e...</td>\n",
       "      <td>[-0.021990731, 0.020082157, 0.017604021, 0.002...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2930367581669767, 0.0036731910177007464, 0....</td>\n",
       "      <td>2019-10-16</td>\n",
       "      <td>14083</td>\n",
       "      <td>0_cart_weed_strain_thc</td>\n",
       "      <td>weed - thc - cart</td>\n",
       "      <td>[cart, weed, strain, thc, bud, price, product,...</td>\n",
       "      <td>[general review template general information d...</td>\n",
       "      <td>[-0.63843, 0.5993336, 2.1778893, -0.6935897, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bg gone either oc look like he even cheaper oc...</td>\n",
       "      <td>[-0.13640997, 0.040470857, -0.0007943742, 0.06...</td>\n",
       "      <td>38</td>\n",
       "      <td>[0.005669327905041996, 0.0035937638895178897, ...</td>\n",
       "      <td>2019-10-16</td>\n",
       "      <td>1499</td>\n",
       "      <td>38_pack_week_day_ordered</td>\n",
       "      <td>order</td>\n",
       "      <td>[pack, week, day, ordered, land, got pack, got...</td>\n",
       "      <td>[confirm ordered pm last week messaged saying ...</td>\n",
       "      <td>[0.5634161, -0.7750787, 2.1607575, -0.37249324...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document  \\\n",
       "0  finally got dream lilxan account confirmed pgp...   \n",
       "1  im issues vendor account issues withdrawing cm...   \n",
       "2  making switch xmr besides xmr hodler favorite ...   \n",
       "3  got free cooky cart order cannacreations one e...   \n",
       "4  bg gone either oc look like he even cheaper oc...   \n",
       "\n",
       "                                           Embedding  Topic  \\\n",
       "0  [-0.05092696, -0.028831957, 0.03425763, -0.008...      9   \n",
       "1  [0.0006082218, 0.061474808, 0.017827673, 0.010...      1   \n",
       "2  [-0.063973725, -0.040487997, -0.025320696, -0....     15   \n",
       "3  [-0.021990731, 0.020082157, 0.017604021, 0.002...      0   \n",
       "4  [-0.13640997, 0.040470857, -0.0007943742, 0.06...     38   \n",
       "\n",
       "                                         Probability  Created_on  Count  \\\n",
       "0  [0.0019145853509105579, 0.0034859457117316834,...  2019-10-16   2462   \n",
       "1  [0.004161525996584431, 0.09864712150395494, 0....  2019-10-30   5365   \n",
       "2  [0.002285542372071366, 0.007466206317565759, 0...  2019-10-16   2995   \n",
       "3  [0.2930367581669767, 0.0036731910177007464, 0....  2019-10-16  14083   \n",
       "4  [0.005669327905041996, 0.0035937638895178897, ...  2019-10-16   1499   \n",
       "\n",
       "                                  Name                            CustomName  \\\n",
       "0  9_pgp_begin pgp_begin_pgp signature  pgp - pgp signature - hash signature   \n",
       "1         1_deposit_address_ticket_btc            ticket - deposit - address   \n",
       "2             15_monero_xmr_wallet_btc                 monero - btc - wallet   \n",
       "3               0_cart_weed_strain_thc                     weed - thc - cart   \n",
       "4             38_pack_week_day_ordered                                 order   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [pgp, begin pgp, begin, pgp signature, signatu...   \n",
       "1  [deposit, address, ticket, btc, wallet, deposi...   \n",
       "2  [monero, xmr, wallet, btc, exchange, bitcoin, ...   \n",
       "3  [cart, weed, strain, thc, bud, price, product,...   \n",
       "4  [pack, week, day, ordered, land, got pack, got...   \n",
       "\n",
       "                                 Representative_Docs  \\\n",
       "0  [vendor links last online fri dec utc last onl...   \n",
       "1  [missing two deposit week ago big deposit erro...   \n",
       "2  [mixing bitcoin old school easy get scammed fe...   \n",
       "3  [general review template general information d...   \n",
       "4  [confirm ordered pm last week messaged saying ...   \n",
       "\n",
       "                                      UMAP_embedding  \n",
       "0  [0.18862869, 1.9764707, 6.3155804, -0.32967466...  \n",
       "1  [0.71821356, -0.18454741, 2.907359, -0.1046811...  \n",
       "2  [0.15870488, 0.024624836, 3.1060727, -0.176952...  \n",
       "3  [-0.63843, 0.5993336, 2.1778893, -0.6935897, 1...  \n",
       "4  [0.5634161, -0.7750787, 2.1607575, -0.37249324...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content = pd.read_parquet('../../Analyze_files/CombiningAnalysisCompleteDataset/ContentAnalysis/DatasetsContentBERTopic/BERTopic_all-MiniLM-L6-v2_190_20n_8dim.parquet')\n",
    "df_content = df_content.dropna()\n",
    "df_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 169288 entries, 0 to 169287\n",
      "Data columns (total 11 columns):\n",
      " #   Column               Non-Null Count   Dtype \n",
      "---  ------               --------------   ----- \n",
      " 0   Document             169288 non-null  object\n",
      " 1   Embedding            169288 non-null  object\n",
      " 2   Topic                169288 non-null  int64 \n",
      " 3   Probability          169288 non-null  object\n",
      " 4   Created_on           169288 non-null  object\n",
      " 5   Count                169288 non-null  int64 \n",
      " 6   Name                 169288 non-null  object\n",
      " 7   CustomName           169288 non-null  object\n",
      " 8   Representation       169288 non-null  object\n",
      " 9   Representative_Docs  169288 non-null  object\n",
      " 10  UMAP_embedding       169288 non-null  object\n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 14.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_content.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05092696, -0.02883196,  0.03425763, ...,  0.00909158,\n",
       "        -0.03866448, -0.04109244],\n",
       "       [ 0.00060822,  0.06147481,  0.01782767, ..., -0.04401757,\n",
       "        -0.02660749, -0.08558437],\n",
       "       [-0.06397372, -0.040488  , -0.0253207 , ..., -0.05395703,\n",
       "        -0.03535156,  0.05759883],\n",
       "       ...,\n",
       "       [-0.07217486,  0.04563387,  0.03032297, ...,  0.09671947,\n",
       "        -0.06746073,  0.03873233],\n",
       "       [-0.13904728, -0.02408719, -0.02283162, ...,  0.04082799,\n",
       "        -0.00130486,  0.04338759],\n",
       "       [ 0.01306497,  0.04508202, -0.00048264, ..., -0.06037715,\n",
       "         0.01438037, -0.04874933]], dtype=float32)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_content[['Embedding', 'Created_on']]\n",
    "embedding_dim = len(X['Embedding'][0])\n",
    "embedding_cols = pd.DataFrame(X['Embedding'].tolist(), index=X.index, columns=[f'embedding_{i}' for i in range(embedding_dim)])\n",
    "X = pd.concat([X, embedding_cols], axis=1)\n",
    "X.drop(columns=['Embedding'], inplace=True)\n",
    "embedding = X.drop(columns=['Created_on']).values\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([608, 622, 608, ..., 493, 537, 484], dtype=int64)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['Created_on'] = pd.to_datetime(X['Created_on'])\n",
    "# Standardize Time Stamp\n",
    "first_date = X['Created_on'].min()\n",
    "X['days_since_start'] = (X['Created_on'] - first_date).dt.days\n",
    "timestamp = X['days_since_start'].values\n",
    "timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9,  1, 15, ..., 89, 19, 17])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df_content['Topic'].astype(int).values\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((135430, 384), (33858, 384), (135430, 1), (33858, 1), (135430,), (33858,))"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_data_train = embedding\n",
    "timestamp_array_train = timestamp\n",
    "labels_train = y\n",
    "X_train_emb, X_test_emb, X_train_timestamp, X_test_timestamp, y_train, y_test = train_test_split(\n",
    "    embedding_data_train, timestamp_array_train, labels_train, test_size=0.2, random_state=42)\n",
    "X_train_ts = X_train_timestamp[:, np.newaxis]\n",
    "X_test_ts = X_test_timestamp[:, np.newaxis]\n",
    "X_train_emb.shape, X_test_emb.shape, X_train_ts.shape, X_test_ts.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_75\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_75\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_76      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_75      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_14         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_76[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_84 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">96,250</span> │ input_layer_75[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_77 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,400</span> │ get_item_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_79          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_84[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_80          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_77[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_13      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_79[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dropout_80[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_85 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">60,200</span> │ concatenate_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_86 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">121</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,321</span> │ dense_85[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_76      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_75      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_14         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input_layer_76[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mGetItem\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_84 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m)       │     \u001b[38;5;34m96,250\u001b[0m │ input_layer_75[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_77 (\u001b[38;5;33mLSTM\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │     \u001b[38;5;34m10,400\u001b[0m │ get_item_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_79          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_84[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_80          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ lstm_77[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_13      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dropout_79[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dropout_80[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_85 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)       │     \u001b[38;5;34m60,200\u001b[0m │ concatenate_13[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_86 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m121\u001b[0m)       │     \u001b[38;5;34m24,321\u001b[0m │ dense_85[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">191,171</span> (746.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m191,171\u001b[0m (746.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">191,171</span> (746.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m191,171\u001b[0m (746.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the embedding input layer\n",
    "embedding_input = Input(shape=(384,))\n",
    "\n",
    "# Define the timestamp input layer\n",
    "timestamp_input = Input(shape=(1,))\n",
    "\n",
    "# Dense layer applied to the embedding input\n",
    "x = Dense(250, activation='relu')(embedding_input)\n",
    "x = Dropout(0.2)(x)\n",
    "# LSTM layer applied to the timestamp input\n",
    "# Add an extra dimension to fit the LSTM input shape requirement\n",
    "y = LSTM(50)(timestamp_input[:, np.newaxis])\n",
    "# Dropout layer to prevent overfitting\n",
    "y = Dropout(0.2)(y)\n",
    "\n",
    "# Concatenate the outputs of the dense and LSTM layers\n",
    "combined = concatenate([x, y])\n",
    "\n",
    "# Additional dense layer with ReLU activation\n",
    "z = Dense(200, activation='relu')(combined)\n",
    "\n",
    "# Output layer with softmax activation for multi-class classification\n",
    "output = Dense(121, activation='softmax')(z)\n",
    "\n",
    "# Define the model with input and output layers\n",
    "model = Model(inputs=[embedding_input, timestamp_input], outputs=output)\n",
    "\n",
    "# Compile the model with Adam optimizer, sparse categorical crossentropy loss,\n",
    "# and metrics including accuracy and sparse categorical accuracy\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy', 'sparse_categorical_accuracy'])\n",
    "\n",
    "# Print a summary of the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint and EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths for saving the model weights\n",
    "checkpoint_path = 'model_checkpoint.weights.h5'\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping_callback = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Model checkpoint callback\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - accuracy: 0.5697 - loss: 1.7643 - sparse_categorical_accuracy: 0.5697 - val_accuracy: 0.8053 - val_loss: 0.6464 - val_sparse_categorical_accuracy: 0.8053\n",
      "Epoch 2/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 6ms/step - accuracy: 0.8039 - loss: 0.6443 - sparse_categorical_accuracy: 0.8039 - val_accuracy: 0.8259 - val_loss: 0.5671 - val_sparse_categorical_accuracy: 0.8259\n",
      "Epoch 3/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - accuracy: 0.8274 - loss: 0.5560 - sparse_categorical_accuracy: 0.8274 - val_accuracy: 0.8361 - val_loss: 0.5401 - val_sparse_categorical_accuracy: 0.8361\n",
      "Epoch 4/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 6ms/step - accuracy: 0.8416 - loss: 0.5006 - sparse_categorical_accuracy: 0.8416 - val_accuracy: 0.8383 - val_loss: 0.5226 - val_sparse_categorical_accuracy: 0.8383\n",
      "Epoch 5/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 6ms/step - accuracy: 0.8555 - loss: 0.4549 - sparse_categorical_accuracy: 0.8555 - val_accuracy: 0.8436 - val_loss: 0.5139 - val_sparse_categorical_accuracy: 0.8436\n",
      "Epoch 6/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - accuracy: 0.8654 - loss: 0.4184 - sparse_categorical_accuracy: 0.8654 - val_accuracy: 0.8444 - val_loss: 0.5093 - val_sparse_categorical_accuracy: 0.8444\n",
      "Epoch 7/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - accuracy: 0.8715 - loss: 0.3927 - sparse_categorical_accuracy: 0.8715 - val_accuracy: 0.8466 - val_loss: 0.5108 - val_sparse_categorical_accuracy: 0.8466\n",
      "Epoch 8/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.8800 - loss: 0.3687 - sparse_categorical_accuracy: 0.8800 - val_accuracy: 0.8460 - val_loss: 0.5038 - val_sparse_categorical_accuracy: 0.8460\n",
      "Epoch 9/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - accuracy: 0.8868 - loss: 0.3427 - sparse_categorical_accuracy: 0.8868 - val_accuracy: 0.8493 - val_loss: 0.5089 - val_sparse_categorical_accuracy: 0.8493\n",
      "Epoch 10/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - accuracy: 0.8919 - loss: 0.3228 - sparse_categorical_accuracy: 0.8919 - val_accuracy: 0.8454 - val_loss: 0.5212 - val_sparse_categorical_accuracy: 0.8454\n",
      "Epoch 11/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - accuracy: 0.8966 - loss: 0.3083 - sparse_categorical_accuracy: 0.8966 - val_accuracy: 0.8487 - val_loss: 0.5214 - val_sparse_categorical_accuracy: 0.8487\n",
      "Epoch 12/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - accuracy: 0.8998 - loss: 0.2971 - sparse_categorical_accuracy: 0.8998 - val_accuracy: 0.8467 - val_loss: 0.5295 - val_sparse_categorical_accuracy: 0.8467\n",
      "Epoch 13/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - accuracy: 0.9038 - loss: 0.2852 - sparse_categorical_accuracy: 0.9038 - val_accuracy: 0.8494 - val_loss: 0.5324 - val_sparse_categorical_accuracy: 0.8494\n",
      "Epoch 14/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - accuracy: 0.9054 - loss: 0.2778 - sparse_categorical_accuracy: 0.9054 - val_accuracy: 0.8483 - val_loss: 0.5476 - val_sparse_categorical_accuracy: 0.8483\n",
      "Epoch 15/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - accuracy: 0.9095 - loss: 0.2642 - sparse_categorical_accuracy: 0.9095 - val_accuracy: 0.8495 - val_loss: 0.5444 - val_sparse_categorical_accuracy: 0.8495\n",
      "Epoch 16/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - accuracy: 0.9128 - loss: 0.2525 - sparse_categorical_accuracy: 0.9128 - val_accuracy: 0.8458 - val_loss: 0.5593 - val_sparse_categorical_accuracy: 0.8458\n",
      "Epoch 17/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - accuracy: 0.9144 - loss: 0.2469 - sparse_categorical_accuracy: 0.9144 - val_accuracy: 0.8471 - val_loss: 0.5678 - val_sparse_categorical_accuracy: 0.8471\n",
      "Epoch 18/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - accuracy: 0.9152 - loss: 0.2407 - sparse_categorical_accuracy: 0.9152 - val_accuracy: 0.8472 - val_loss: 0.5729 - val_sparse_categorical_accuracy: 0.8472\n",
      "Epoch 19/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - accuracy: 0.9182 - loss: 0.2337 - sparse_categorical_accuracy: 0.9182 - val_accuracy: 0.8475 - val_loss: 0.5762 - val_sparse_categorical_accuracy: 0.8475\n",
      "Epoch 20/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - accuracy: 0.9220 - loss: 0.2244 - sparse_categorical_accuracy: 0.9220 - val_accuracy: 0.8489 - val_loss: 0.5858 - val_sparse_categorical_accuracy: 0.8489\n",
      "Epoch 21/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - accuracy: 0.9259 - loss: 0.2167 - sparse_categorical_accuracy: 0.9259 - val_accuracy: 0.8460 - val_loss: 0.5991 - val_sparse_categorical_accuracy: 0.8460\n",
      "Epoch 22/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - accuracy: 0.9258 - loss: 0.2142 - sparse_categorical_accuracy: 0.9258 - val_accuracy: 0.8438 - val_loss: 0.6178 - val_sparse_categorical_accuracy: 0.8438\n",
      "Epoch 23/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - accuracy: 0.9282 - loss: 0.2066 - sparse_categorical_accuracy: 0.9282 - val_accuracy: 0.8455 - val_loss: 0.6231 - val_sparse_categorical_accuracy: 0.8455\n",
      "Epoch 24/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9291 - loss: 0.2044 - sparse_categorical_accuracy: 0.9291 - val_accuracy: 0.8451 - val_loss: 0.6257 - val_sparse_categorical_accuracy: 0.8451\n",
      "Epoch 25/100\n",
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 6ms/step - accuracy: 0.9302 - loss: 0.2010 - sparse_categorical_accuracy: 0.9302 - val_accuracy: 0.8458 - val_loss: 0.6285 - val_sparse_categorical_accuracy: 0.8458\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_train_emb, X_train_ts],  # input emebdding and timestamp\n",
    "    y_train,  # target\n",
    "    epochs=100,  # number of epochs\n",
    "    batch_size=32,  # batch size\n",
    "    validation_data=([X_test_emb, X_test_ts], y_test),\n",
    "    callbacks=[early_stopping_callback, model_checkpoint_callback]  # validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1059/1059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict([X_test_emb, X_test_ts])\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Validation Data\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.92      0.92      0.92      2811\n",
      "     Class 1       0.92      0.94      0.93      1095\n",
      "     Class 2       0.93      0.92      0.92      1112\n",
      "     Class 3       0.88      0.88      0.88       665\n",
      "     Class 4       0.91      0.93      0.92       685\n",
      "     Class 5       0.74      0.90      0.81       936\n",
      "     Class 6       0.82      0.93      0.87       686\n",
      "     Class 7       0.89      0.90      0.90       592\n",
      "     Class 8       0.94      0.89      0.91       701\n",
      "     Class 9       0.98      0.98      0.98       502\n",
      "    Class 10       0.91      0.91      0.91       499\n",
      "    Class 11       0.80      0.88      0.84       529\n",
      "    Class 12       0.96      0.92      0.94       427\n",
      "    Class 13       0.93      0.92      0.92       377\n",
      "    Class 14       0.83      0.86      0.85       443\n",
      "    Class 15       0.88      0.92      0.90       585\n",
      "    Class 16       0.93      0.96      0.95       365\n",
      "    Class 17       0.91      0.87      0.89       375\n",
      "    Class 18       0.84      0.92      0.88       339\n",
      "    Class 19       0.81      0.82      0.81       397\n",
      "    Class 20       0.87      0.82      0.85       488\n",
      "    Class 21       0.78      0.89      0.83       387\n",
      "    Class 22       0.87      0.86      0.86       331\n",
      "    Class 23       0.75      0.90      0.82       344\n",
      "    Class 24       0.98      0.96      0.97       236\n",
      "    Class 25       0.76      0.87      0.81       729\n",
      "    Class 26       0.84      0.84      0.84       554\n",
      "    Class 27       0.83      0.90      0.86       245\n",
      "    Class 28       0.91      0.94      0.92       275\n",
      "    Class 29       0.91      0.93      0.92       243\n",
      "    Class 30       0.92      0.93      0.93       255\n",
      "    Class 31       0.92      0.90      0.91       252\n",
      "    Class 32       0.89      0.98      0.93       210\n",
      "    Class 33       0.94      0.87      0.90       238\n",
      "    Class 34       0.88      0.86      0.87       204\n",
      "    Class 35       0.92      0.90      0.91       226\n",
      "    Class 36       0.87      0.85      0.86       331\n",
      "    Class 37       0.84      0.82      0.83       238\n",
      "    Class 38       0.88      0.86      0.87       306\n",
      "    Class 39       0.81      0.86      0.84       221\n",
      "    Class 40       0.92      0.92      0.92       189\n",
      "    Class 41       0.98      0.92      0.95       199\n",
      "    Class 42       0.88      0.81      0.84       198\n",
      "    Class 43       0.83      0.86      0.85       204\n",
      "    Class 44       0.84      0.89      0.86       217\n",
      "    Class 45       0.76      0.73      0.74       304\n",
      "    Class 46       0.89      0.81      0.85       237\n",
      "    Class 47       0.80      0.80      0.80       374\n",
      "    Class 48       0.84      0.89      0.87       145\n",
      "    Class 49       0.96      0.92      0.94       140\n",
      "    Class 50       0.84      0.86      0.85       217\n",
      "    Class 51       0.94      0.90      0.92       153\n",
      "    Class 52       0.83      0.71      0.77       242\n",
      "    Class 53       0.81      0.88      0.84       237\n",
      "    Class 54       0.92      0.83      0.87       175\n",
      "    Class 55       0.85      0.90      0.87       182\n",
      "    Class 56       0.86      0.85      0.86       144\n",
      "    Class 57       0.95      0.88      0.91       121\n",
      "    Class 58       0.88      0.89      0.88       134\n",
      "    Class 59       0.80      0.78      0.79       161\n",
      "    Class 60       0.89      0.79      0.84       181\n",
      "    Class 61       0.94      0.95      0.94       116\n",
      "    Class 62       0.80      0.82      0.81       218\n",
      "    Class 63       0.94      0.83      0.88       126\n",
      "    Class 64       0.85      0.82      0.84       125\n",
      "    Class 65       0.99      0.93      0.96       107\n",
      "    Class 66       0.87      0.83      0.85       130\n",
      "    Class 67       0.81      0.73      0.76       264\n",
      "    Class 68       0.86      0.92      0.89        98\n",
      "    Class 69       0.95      0.80      0.87       103\n",
      "    Class 70       0.85      0.86      0.86       143\n",
      "    Class 71       0.86      0.86      0.86       170\n",
      "    Class 72       0.81      0.90      0.85        97\n",
      "    Class 73       0.89      0.77      0.83       132\n",
      "    Class 74       0.72      0.69      0.70       215\n",
      "    Class 75       0.80      0.76      0.78       386\n",
      "    Class 76       0.79      0.75      0.77       100\n",
      "    Class 77       0.86      0.92      0.89        73\n",
      "    Class 78       0.81      0.75      0.78       179\n",
      "    Class 79       0.81      0.85      0.83       104\n",
      "    Class 80       0.81      0.73      0.77       211\n",
      "    Class 81       0.69      0.79      0.74        66\n",
      "    Class 82       0.96      0.91      0.93        76\n",
      "    Class 83       0.81      0.67      0.73       115\n",
      "    Class 84       0.76      0.82      0.79       256\n",
      "    Class 85       0.98      0.86      0.92        66\n",
      "    Class 86       0.83      0.80      0.82        81\n",
      "    Class 87       0.83      0.83      0.83        63\n",
      "    Class 88       0.77      0.72      0.74       146\n",
      "    Class 89       0.70      0.70      0.70       290\n",
      "    Class 90       0.93      0.79      0.86       106\n",
      "    Class 91       0.85      0.86      0.85        91\n",
      "    Class 92       0.67      0.53      0.59       460\n",
      "    Class 93       0.81      0.69      0.74       118\n",
      "    Class 94       0.98      0.80      0.88        70\n",
      "    Class 95       0.60      0.60      0.60       137\n",
      "    Class 96       0.83      0.82      0.82        76\n",
      "    Class 97       0.71      0.59      0.65       400\n",
      "    Class 98       0.86      0.76      0.81        97\n",
      "    Class 99       0.70      0.69      0.70       632\n",
      "   Class 100       0.86      0.76      0.80       222\n",
      "   Class 101       0.92      0.87      0.89        52\n",
      "   Class 102       0.84      0.76      0.80        85\n",
      "   Class 103       0.76      0.94      0.84        47\n",
      "   Class 104       0.71      0.81      0.75        72\n",
      "   Class 105       0.88      0.67      0.76       529\n",
      "   Class 106       0.90      0.88      0.89        49\n",
      "   Class 107       0.98      0.94      0.96        63\n",
      "   Class 108       0.85      0.75      0.80        71\n",
      "   Class 109       0.87      0.58      0.70       106\n",
      "   Class 110       0.75      0.85      0.80        94\n",
      "   Class 111       0.89      0.78      0.83        81\n",
      "   Class 112       0.94      0.87      0.91        39\n",
      "   Class 113       0.86      0.92      0.89        52\n",
      "   Class 114       0.61      0.59      0.60       199\n",
      "   Class 115       0.87      0.93      0.90        42\n",
      "   Class 116       0.61      0.74      0.67       336\n",
      "   Class 117       0.93      0.89      0.91        56\n",
      "   Class 118       0.91      0.95      0.93        42\n",
      "   Class 119       0.66      0.61      0.63       495\n",
      "   Class 120       0.57      0.60      0.58        95\n",
      "\n",
      "    accuracy                           0.85     33858\n",
      "   macro avg       0.85      0.84      0.84     33858\n",
      "weighted avg       0.85      0.85      0.85     33858\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_pred, target_names=[f'Class {i}' for i in range(121)])\n",
    "print(\"Classification Report for Validation Data\\n\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total AUC Score Validation Data: 0.9982\n"
     ]
    }
   ],
   "source": [
    "y_test_binarized = label_binarize(y_test, classes=np.arange(121))\n",
    "auc_score = roc_auc_score(y_test_binarized, y_pred_proba, average=\"macro\", multi_class=\"ovr\")\n",
    "print(f'Total AUC Score Validation Data: {auc_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4233/4233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step\n",
      "Classification Report for Training Set\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.98      0.98      0.98     11272\n",
      "     Class 1       0.98      0.99      0.98      4270\n",
      "     Class 2       0.99      0.98      0.98      4584\n",
      "     Class 3       0.97      0.96      0.97      2703\n",
      "     Class 4       0.96      0.99      0.97      2502\n",
      "     Class 5       0.90      0.99      0.94      3648\n",
      "     Class 6       0.93      0.99      0.96      2739\n",
      "     Class 7       0.96      0.97      0.97      2254\n",
      "     Class 8       0.99      0.97      0.98      2582\n",
      "     Class 9       0.99      0.99      0.99      1960\n",
      "    Class 10       0.98      0.98      0.98      2063\n",
      "    Class 11       0.93      0.97      0.95      2163\n",
      "    Class 12       0.99      0.97      0.98      1673\n",
      "    Class 13       0.99      0.98      0.99      1593\n",
      "    Class 14       0.97      0.98      0.97      1771\n",
      "    Class 15       0.97      0.98      0.98      2410\n",
      "    Class 16       0.97      1.00      0.99      1408\n",
      "    Class 17       0.97      0.97      0.97      1550\n",
      "    Class 18       0.95      0.98      0.96      1548\n",
      "    Class 19       0.94      0.95      0.94      1545\n",
      "    Class 20       0.96      0.94      0.95      1862\n",
      "    Class 21       0.90      0.98      0.94      1649\n",
      "    Class 22       0.97      0.97      0.97      1319\n",
      "    Class 23       0.91      0.98      0.94      1458\n",
      "    Class 24       0.99      0.99      0.99       992\n",
      "    Class 25       0.90      0.95      0.93      3049\n",
      "    Class 26       0.96      0.97      0.97      2275\n",
      "    Class 27       0.95      0.99      0.97       997\n",
      "    Class 28       0.98      0.99      0.98      1056\n",
      "    Class 29       0.98      0.98      0.98       971\n",
      "    Class 30       0.98      0.99      0.99       961\n",
      "    Class 31       0.98      0.98      0.98       974\n",
      "    Class 32       0.98      1.00      0.99       916\n",
      "    Class 33       0.98      0.97      0.98       871\n",
      "    Class 34       0.98      0.98      0.98       870\n",
      "    Class 35       0.98      0.97      0.98       935\n",
      "    Class 36       0.98      0.95      0.96      1364\n",
      "    Class 37       0.97      0.95      0.96      1094\n",
      "    Class 38       0.96      0.97      0.96      1193\n",
      "    Class 39       0.95      0.98      0.97       936\n",
      "    Class 40       0.98      1.00      0.99       734\n",
      "    Class 41       1.00      0.96      0.98       650\n",
      "    Class 42       0.98      0.93      0.96       770\n",
      "    Class 43       0.97      0.98      0.98       806\n",
      "    Class 44       0.92      0.98      0.95       823\n",
      "    Class 45       0.95      0.91      0.93      1377\n",
      "    Class 46       0.98      0.96      0.97       842\n",
      "    Class 47       0.95      0.93      0.94      1476\n",
      "    Class 48       0.97      1.00      0.99       606\n",
      "    Class 49       0.99      0.99      0.99       600\n",
      "    Class 50       0.95      0.98      0.97       824\n",
      "    Class 51       0.99      0.98      0.98       603\n",
      "    Class 52       0.97      0.94      0.95       859\n",
      "    Class 53       0.95      0.98      0.96       953\n",
      "    Class 54       0.96      0.94      0.95       704\n",
      "    Class 55       0.95      0.96      0.95       781\n",
      "    Class 56       0.96      0.98      0.97       619\n",
      "    Class 57       0.98      0.99      0.99       575\n",
      "    Class 58       0.98      0.97      0.98       557\n",
      "    Class 59       0.98      0.96      0.97       669\n",
      "    Class 60       0.97      0.94      0.95       679\n",
      "    Class 61       0.99      0.99      0.99       430\n",
      "    Class 62       0.95      0.94      0.95       836\n",
      "    Class 63       1.00      0.97      0.99       462\n",
      "    Class 64       0.98      0.98      0.98       572\n",
      "    Class 65       1.00      0.98      0.99       419\n",
      "    Class 66       0.97      0.97      0.97       552\n",
      "    Class 67       0.95      0.93      0.94      1146\n",
      "    Class 68       0.98      1.00      0.99       385\n",
      "    Class 69       1.00      0.96      0.98       405\n",
      "    Class 70       0.95      0.99      0.97       538\n",
      "    Class 71       0.97      0.98      0.98       569\n",
      "    Class 72       0.95      1.00      0.97       419\n",
      "    Class 73       0.99      0.91      0.95       466\n",
      "    Class 74       0.93      0.89      0.91       929\n",
      "    Class 75       0.92      0.90      0.91      1430\n",
      "    Class 76       0.96      0.97      0.96       442\n",
      "    Class 77       0.99      1.00      1.00       319\n",
      "    Class 78       0.97      0.96      0.96       696\n",
      "    Class 79       0.97      0.96      0.96       409\n",
      "    Class 80       0.93      0.89      0.91       867\n",
      "    Class 81       0.94      1.00      0.97       291\n",
      "    Class 82       0.99      1.00      0.99       280\n",
      "    Class 83       0.99      0.90      0.94       434\n",
      "    Class 84       0.92      0.95      0.93      1015\n",
      "    Class 85       1.00      0.97      0.99       272\n",
      "    Class 86       0.97      0.97      0.97       320\n",
      "    Class 87       0.99      1.00      0.99       268\n",
      "    Class 88       0.96      0.92      0.94       602\n",
      "    Class 89       0.89      0.94      0.91      1079\n",
      "    Class 90       0.99      0.94      0.96       407\n",
      "    Class 91       0.97      0.98      0.98       308\n",
      "    Class 92       0.92      0.82      0.87      1780\n",
      "    Class 93       0.99      0.94      0.96       429\n",
      "    Class 94       1.00      0.97      0.99       237\n",
      "    Class 95       0.85      0.87      0.86       538\n",
      "    Class 96       0.98      0.99      0.98       303\n",
      "    Class 97       0.93      0.77      0.84      1637\n",
      "    Class 98       0.97      0.92      0.95       396\n",
      "    Class 99       0.94      0.91      0.93      2444\n",
      "   Class 100       0.96      0.91      0.93       916\n",
      "   Class 101       0.99      0.99      0.99       244\n",
      "   Class 102       0.96      0.95      0.95       338\n",
      "   Class 103       0.97      0.99      0.98       260\n",
      "   Class 104       0.93      0.96      0.94       283\n",
      "   Class 105       0.98      0.82      0.89      1931\n",
      "   Class 106       1.00      1.00      1.00       200\n",
      "   Class 107       0.99      0.99      0.99       203\n",
      "   Class 108       0.99      0.91      0.95       330\n",
      "   Class 109       0.99      0.87      0.93       492\n",
      "   Class 110       0.93      0.98      0.95       382\n",
      "   Class 111       0.94      0.93      0.94       317\n",
      "   Class 112       0.99      0.99      0.99       185\n",
      "   Class 113       0.98      0.97      0.97       238\n",
      "   Class 114       0.88      0.83      0.86       865\n",
      "   Class 115       0.99      1.00      1.00       172\n",
      "   Class 116       0.83      0.94      0.88      1418\n",
      "   Class 117       0.99      0.97      0.98       162\n",
      "   Class 118       0.99      0.98      0.98       168\n",
      "   Class 119       0.95      0.87      0.91      2094\n",
      "   Class 120       0.91      0.84      0.87       414\n",
      "\n",
      "    accuracy                           0.96    135430\n",
      "   macro avg       0.96      0.96      0.96    135430\n",
      "weighted avg       0.96      0.96      0.96    135430\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_pred_proba = model.predict([X_train_emb, X_train_ts])\n",
    "y_train_pred = np.argmax(y_train_pred_proba, axis=1)\n",
    "train_report = classification_report(y_train, y_train_pred, target_names=[f'Class {i}' for i in range(121)])\n",
    "print(\"Classification Report for Training Set\\n\")\n",
    "print(train_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total AUC Score Training Data: 0.9999\n"
     ]
    }
   ],
   "source": [
    "y_train_binarized = label_binarize(y_train, classes=np.arange(121))\n",
    "train_auc_score = roc_auc_score(y_train_binarized, y_train_pred_proba, average=\"macro\", multi_class=\"ovr\")\n",
    "print(f'Total AUC Score Training Data: {train_auc_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('LSTM_content.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTILABEL MULTICLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_77\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_77\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_78      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_15         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_78[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_77      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_78 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,400</span> │ get_item_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_87 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">57,750</span> │ input_layer_77[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_81          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_78[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_14      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_87[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dropout_81[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_88 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">40,200</span> │ concatenate_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_89 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">121</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,321</span> │ dense_88[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_78      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_15         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input_layer_78[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mGetItem\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_77      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_78 (\u001b[38;5;33mLSTM\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │     \u001b[38;5;34m10,400\u001b[0m │ get_item_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_87 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)       │     \u001b[38;5;34m57,750\u001b[0m │ input_layer_77[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_81          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ lstm_78[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_14      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_87[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dropout_81[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_88 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)       │     \u001b[38;5;34m40,200\u001b[0m │ concatenate_14[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_89 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m121\u001b[0m)       │     \u001b[38;5;34m24,321\u001b[0m │ dense_88[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">132,671</span> (518.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m132,671\u001b[0m (518.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">132,671</span> (518.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m132,671\u001b[0m (518.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the embedding input layer\n",
    "embedding_input = Input(shape=(384,))\n",
    "\n",
    "# Define the timestamp input layer\n",
    "timestamp_input = Input(shape=(1,))\n",
    "\n",
    "# Dense layer applied to the embedding input\n",
    "x = Dense(150, activation='relu')(embedding_input)\n",
    "\n",
    "# LSTM layer applied to the timestamp input\n",
    "# Add an extra dimension to fit the LSTM input shape requirement\n",
    "y = LSTM(50)(timestamp_input[:, np.newaxis])\n",
    "# Dropout layer to prevent overfitting\n",
    "y = Dropout(0.2)(y)\n",
    "\n",
    "# Concatenate the outputs of the dense and LSTM layers\n",
    "combined = concatenate([x, y])\n",
    "\n",
    "# Additional dense layer with ReLU activation\n",
    "z = Dense(200, activation='relu')(combined)\n",
    "\n",
    "# Output layer with sigmoid activation for multilabel classification\n",
    "output = Dense(121, activation='sigmoid')(z)\n",
    "\n",
    "# Define the model with input and output layers\n",
    "model = Model(inputs=[embedding_input, timestamp_input], outputs=output)\n",
    "\n",
    "# Compile the model with Adam optimizer and binary crossentropy loss\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print a summary of the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, 121)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[352], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m model_checkpoint_callback \u001b[38;5;241m=\u001b[39m ModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_checkpoint.weights.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      3\u001b[0m                                             monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_ts\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# input for embeddings and time series\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# multilabel binary targets\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# number of epochs\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# batch size\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_test_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_ts\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# validation data\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# callbacks\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     16\u001b[0m train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate([X_train_emb, X_train_ts], y_train)\n",
      "File \u001b[1;32mc:\\Users\\dommy\\miniconda3\\envs\\gestione\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\dommy\\miniconda3\\envs\\gestione\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:656\u001b[0m, in \u001b[0;36mbinary_crossentropy\u001b[1;34m(target, output, from_logits)\u001b[0m\n\u001b[0;32m    653\u001b[0m output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(output)\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[1;32m--> 656\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    657\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must have the same rank \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    658\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(ndim). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    659\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    660\u001b[0m     )\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m    662\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n",
      "\u001b[1;31mValueError\u001b[0m: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, 121)"
     ]
    }
   ],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath='model_checkpoint.weights.h5', save_weights_only=True,\n",
    "                                            monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_train_emb, X_train_ts],  # input for embeddings and time series\n",
    "    y_train,  # multilabel binary targets\n",
    "    epochs=10,  # number of epochs\n",
    "    batch_size=64,  # batch size\n",
    "    validation_data=([X_test_emb, X_test_ts], y_test),  # validation data\n",
    "    callbacks=[early_stopping_callback, model_checkpoint_callback]  # callbacks\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "train_loss, train_accuracy = model.evaluate([X_train_emb, X_train_ts], y_train)\n",
    "test_loss, test_accuracy = model.evaluate([X_test_emb, X_test_ts], y_test)\n",
    "\n",
    "print(f'Training loss: {train_loss}, Training accuracy: {train_accuracy}')\n",
    "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gestione",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
