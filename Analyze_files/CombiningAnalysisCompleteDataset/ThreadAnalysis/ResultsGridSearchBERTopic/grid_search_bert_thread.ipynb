{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dommy\\miniconda3\\envs\\gestione\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../../../../Util')\n",
    "import TextClustering as tc\n",
    "import BERTopicUtils as btu\n",
    "from tqdm import tqdm\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "import hdbscan\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import csv\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66735\n"
     ]
    }
   ],
   "source": [
    "df = btu.load_data_filtered('../cleaned_data_name_thread.csv', 'name_thread')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 13:49:43,558 - TextClustering - INFO - Encoding the corpus. This might take a while.\n",
      "Batches:  52%|█████▏    | 536/1024 [06:31<03:22,  2.40it/s]"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "tc1 = tc.TextClustering(df, 'name_thread')\n",
    "tc1.encode_corpus(model, batch_size=64, to_tensor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "umap_model = UMAP(n_neighbors=15, n_components=10, min_dist=0.0, metric='cosine', random_state=42)\n",
    "hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=300, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    top_n_words=10, \n",
    "    n_gram_range=(1, 2),\n",
    "    umap_model=umap_model, \n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model, \n",
    "    ctfidf_model=ctfidf_model, \n",
    "    representation_model=representation_model,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(tc1.corpus, tc1.corpus_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_meet_criteria(topic_model: BERTopic, check_gun_topic: bool = False) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the topics meet the criteria for the assignment.\n",
    "    :param topic_model: The BERTopic model used for clustering\n",
    "    :param check_gun_topic: Whether to check for the gun topic\n",
    "    :return: True if the topics meet the criteria, False otherwise\n",
    "    \"\"\"\n",
    "    topics = topic_model.get_topics()\n",
    "    drug_keywords = {'drug', 'cocaine', 'ketamine', 'weed', 'mdma', 'coke', 'lsd', 'heroine', 'xanax'}\n",
    "    scam_bitcoin_keywords = {'transfer', 'bitcoin', 'wallet', 'paypal', 'bank'}\n",
    "    market_keywords = {'market', 'vendor', 'darknet', 'scammer', 'scammed'}\n",
    "    hacker_keywords = {'hacker', 'hack', 'exploit', 'exploited', 'exploitation', 'exploiting', 'exploiter', 'exploits'}\n",
    "    gun_keywords = {'gun', 'firearm', 'pistol', 'rifle', 'shooting', 'weapon', 'handgun'}\n",
    "    \n",
    "    found_drug_topic = found_scam_bitcoin_topic = found_market_topic = found_hacker_topic = False\n",
    "    drug_topic_id = scam_bitcoin_topic_id = market_topic_id = hacker_topic_id = None\n",
    "\n",
    "    for topic_id, topic in topics.items():\n",
    "        words = set([word for word, _ in topic])\n",
    "        if words & drug_keywords:\n",
    "            found_drug_topic = True\n",
    "            drug_topic_id = topic_id\n",
    "        if words & scam_bitcoin_keywords:\n",
    "            found_scam_bitcoin_topic = True\n",
    "            scam_bitcoin_topic_id = topic_id\n",
    "        if words & market_keywords:\n",
    "            found_market_topic = True\n",
    "            market_topic_id = topic_id\n",
    "        if words & hacker_keywords:\n",
    "            found_hacker_topic = True\n",
    "            hacker_topic_id = topic_id\n",
    "\n",
    "    distinct_clusters = len(set(filter(None, [drug_topic_id, scam_bitcoin_topic_id, market_topic_id, hacker_topic_id])))\n",
    "\n",
    "    if check_gun_topic:\n",
    "        return (found_drug_topic and found_scam_bitcoin_topic and found_market_topic and found_hacker_topic and \n",
    "                distinct_clusters >= 4)\n",
    "    \n",
    "    return (found_drug_topic and found_scam_bitcoin_topic and found_market_topic and \n",
    "            distinct_clusters >= 3)\n",
    "\n",
    "def cluster_size_within_threshold(topic_model: BERTopic, threshold: float = 0.15) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the sizes of the top 4 clusters are within the threshold.\n",
    "    :param topic_model: The BERTopic model used for clustering\n",
    "    :param threshold: The threshold for the size difference between the top 4 clusters\n",
    "    :return: True if the sizes are within the threshold, False otherwise\n",
    "    \"\"\"\n",
    "    topic_freq = topic_model.get_topic_freq()\n",
    "    top_4_clusters = topic_freq.iloc[1:5]  # First row is for outliers\n",
    "\n",
    "    for i in range(len(top_4_clusters) - 1):\n",
    "        size_diff = abs(top_4_clusters.iloc[i][\"Count\"] - top_4_clusters.iloc[i + 1][\"Count\"])\n",
    "        max_size = max(top_4_clusters.iloc[i][\"Count\"], top_4_clusters.iloc[i + 1][\"Count\"])\n",
    "        if size_diff / max_size > threshold:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def compute_clustering_scores(topic_model: BERTopic, topics: list, embeddings: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute the silhouette score and Davies-Bouldin score for the clustering results.\n",
    "    :param topic_model: The BERTopic model used for clustering\n",
    "    :param topics: The topics assigned to each document\n",
    "    :param embeddings: The embeddings of the documents\n",
    "    :return: The silhouette score and Davies-Bouldin score\n",
    "    \"\"\"\n",
    "    # Filter out outliers (cluster -1)\n",
    "    valid_indices = [i for i, topic in enumerate(topics) if topic != -1]\n",
    "    valid_embeddings = np.array([embeddings[i] for i in valid_indices])\n",
    "    valid_topics = np.array([topics[i] for i in valid_indices])\n",
    "    \n",
    "    if len(set(valid_topics)) > 1:  # At least two clusters are needed to compute these scores\n",
    "        silhouette = silhouette_score(valid_embeddings, valid_topics)\n",
    "        davies_bouldin = davies_bouldin_score(valid_embeddings, valid_topics)\n",
    "        return silhouette, davies_bouldin\n",
    "    else:\n",
    "        return -1, float('inf')\n",
    "        \n",
    "def grid_search_clusters(initial_model: BERTopic, initial_topics: list, \n",
    "                         log_file: str = \"grid_search_log_hdbscan_min_cluster.csv\", target_outliers: int = 25000, \n",
    "                         max_iters: int = 10, cluster_size_threshold: float = 0.20, min_silhouette: float = 0.3) -> tuple:\n",
    "    \"\"\"\n",
    "    Perform a grid search to find the optimal parameters for the BERTopic model.\n",
    "    :param initial_model: The initial BERTopic model to start the search from\n",
    "    :param initial_topics: The initial topics assigned by the model\n",
    "    :param target_outliers: The target number of outliers to have in the final model\n",
    "    :param max_iters: The maximum number of iterations to run the grid search for\n",
    "    :param cluster_size_threshold: The threshold for the difference in cluster sizes\n",
    "    :param min_silhouette: The minimum acceptable silhouette score\n",
    "    :return: The best BERTopic model, the topics assigned by the model, and the probabilities of each topic\n",
    "    \"\"\"\n",
    "    best_model = initial_model\n",
    "    best_topics = initial_topics\n",
    "    best_probs = None\n",
    "\n",
    "    # Define parameter ranges for grid search\n",
    "    n_neighbors_range = [5, 10, 15, 30, 50, 80]\n",
    "    n_components_range = [2, 3, 5, 10]\n",
    "    min_cluster_size_range = [200, 300, 500, 600, 1000, 1100, 1200, 1300, 1400]\n",
    "    \n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "    representation_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "    zeroshot_topic_list = pd.read_csv('../../../../Datasets/IntentCrime/intent_crime.csv')['intent'].tolist()\n",
    "\n",
    "    iteration = 0\n",
    "    log_file = log_file\n",
    "    \n",
    "    # Open log file and write headers\n",
    "    with open(log_file, mode='w', newline='') as log_csv:\n",
    "        log_writer = csv.writer(log_csv)\n",
    "        log_writer.writerow([\"min_cluster_size\", \"outliers\", \"silhouette\", \"davies_bouldin\", \"meets_criteria\", \"cluster_sizes\", \"representative_words\"])\n",
    "\n",
    "    # Start the grid search\n",
    "    print(f\"Iteration {iteration}: Adjusting parameters...\")\n",
    "\n",
    "    for min_clu in tqdm(min_cluster_size_range, desc=\"min_cluster_size\"):\n",
    "        # Configure UMAP model with current parameters\n",
    "        umap_model = UMAP(n_neighbors=15, n_components=10, metric='cosine', random_state=42)\n",
    "        \n",
    "        hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=min_clu, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "        \n",
    "        # Recreate BERTopic model with new parameters\n",
    "        topic_model = BERTopic(\n",
    "            top_n_words=10, \n",
    "            n_gram_range=(1, 2),\n",
    "            umap_model=umap_model, \n",
    "            hdbscan_model=hdbscan_model, \n",
    "            vectorizer_model=vectorizer_model, \n",
    "            ctfidf_model=ctfidf_model, \n",
    "            representation_model=representation_model,\n",
    "            zeroshot_topic_list=zeroshot_topic_list, \n",
    "            zeroshot_min_similarity=.1, \n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Refit the model\n",
    "        topics, probs = topic_model.fit_transform(tc1.corpus, tc1.corpus_embeddings)\n",
    "        \n",
    "        # Calculate the number of outliers\n",
    "        outlier_count = sum([1 if topic == -1 else 0 for topic in topics])\n",
    "        \n",
    "        # Compute clustering scores\n",
    "        print(\"Computing clustering scores...\")\n",
    "        umap_embeddings = topic_model.umap_model.fit_transform(tc1.corpus_embeddings)\n",
    "        indices = [index for index, topic in enumerate(topics) if topic != -1]\n",
    "        X = umap_embeddings[np.array(indices)]\n",
    "        labels = [topic for index, topic in enumerate(topics) if topic != -1]\n",
    "        silhouette = silhouette_score(X, labels)\n",
    "        davies_bouldin = davies_bouldin_score(X, labels)\n",
    "        \n",
    "        # Check if the configuration meets the criteria\n",
    "        meets_criteria = topics_meet_criteria(topic_model)\n",
    "        #cluster_size_within_threshold_flag = cluster_size_within_threshold(topic_model, cluster_size_threshold)\n",
    "        \n",
    "        # Get cluster sizes and representative words\n",
    "        topic_info = topic_model.get_topic_info()\n",
    "        cluster_sizes = topic_info['Count'].to_list()\n",
    "        representative_words = [topic_model.get_topic(topic_id) for topic_id in topic_info['Topic'].to_list()]\n",
    "        \n",
    "        # Format representative words\n",
    "        formatted_representative_words = [\" \".join([word for word, _ in words]) for words in representative_words]\n",
    "\n",
    "        # Log the current configuration\n",
    "        with open(log_file, mode='a', newline='') as log_csv:\n",
    "            log_writer = csv.writer(log_csv)\n",
    "            log_writer.writerow([min_clu, outlier_count, silhouette, davies_bouldin, meets_criteria, cluster_sizes, formatted_representative_words])\n",
    "                \n",
    "    return best_model, best_topics, best_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model, topics, probs = grid_search_clusters(topic_model, topics, log_file=\"grid_search_log_hdbscan_MiniLM.csv\", target_outliers=25000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gestione",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
